

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Why should I use openspeech? &mdash; Openspeech v0.1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Openspeech’s Hydra configuration" href="hydra_configs.html" />
    <link rel="prev" title="Welcome to Openspeech’s documentation!" href="../index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Openspeech
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">GETTING STARTED</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Why should I use openspeech?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#why-shouldn-t-i-use-openspeech">Why shouldn’t I use openspeech?</a></li>
<li class="toctree-l1"><a class="reference internal" href="#model-architectures">Model architectures</a></li>
<li class="toctree-l1"><a class="reference internal" href="#get-started">Get Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#supported-datasets">Supported Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="#manifest-file">Manifest File</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-examples">Training examples</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#installation">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l2"><a class="reference internal" href="#install-from-source">Install from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="#install-apex-for-16-bit-training">Install Apex (for 16-bit training)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#troubleshoots-and-contributing">Troubleshoots and Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#code-style">Code Style</a></li>
<li class="toctree-l2"><a class="reference internal" href="#license">License</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#citation">Citation</a></li>
<li class="toctree-l1"><a class="reference internal" href="hydra_configs.html">Openspeech’s Hydra configuration</a></li>
<li class="toctree-l1"><a class="reference internal" href="configs.html">Openspeech’s configurations</a></li>
</ul>
<p class="caption"><span class="caption-text">MODEL ARCHITECTURES</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Openspeech Model.html">Openspeech Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Conformer.html">Conformer Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Conformer LSTM.html">Conformer LSTM Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Conformer Transducer.html">Conformer Transducer Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Deep CNN with Joint CTC LAS.html">Deep CNN with Joint CTC LAS Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../DeepSpeech2.html">DeepSpeech2 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Jasper5x3.html">Jasper5x3 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Jasper10x5.html">Jasper10x5 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Joint CTC Conformer LSTM.html">Joint CTC Conformer LSTM Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Joint CTC LAS.html">Joint CTC LAS Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Joint CTC Transformer.html">Joint CTC Transformer Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Listen Attend Spell.html">Listen Attend Spell Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Listen Attend Spell (location-aware).html">Listen Attend Spell (location-aware) Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Listen Attend Spell (multi-head).html">Listen Attend Spell (multi-head) Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../QuartzNet5x5.html">QuartzNet 5x5 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../QuartzNet10x5.html">QuartzNet 10x5 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../QuartzNet15x5.html">QuartzNet 15x5 Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../RNN Transducer.html">RNN Transducer Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Transformer.html">Transformer Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Transformer Transducer.html">Transformer Transducer Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Transformer with CTC.html">Transformer with CTC Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../VGG Transformer.html">VGG Transformer Model</a></li>
</ul>
<p class="caption"><span class="caption-text">LIBRARY REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Audio.html">Audio</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Criterion.html">Criterion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Data.html">Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Datasets.html">Datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Decoders.html">Decoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Encoders.html">Encoders</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Optim.html">Optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Beam Search.html">Search</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Vocabulary.html">Vocabulary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Metric.html">Metric</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Openspeech</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Why should I use openspeech?</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/notes/intro.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <p>Openspeech provides reference implementations of various ASR modeling papers and three languages recipe to perform tasks on automatic speech recognition. Our aim is to make ASR technology easier to use for everyone.</p>
<p>Openspeech is backed by the two powerful libraries — <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning">PyTorch-Lightning</a> and <a class="reference external" href="https://github.com/facebookresearch/hydra">Hydra</a>.
Various features are available in the above two libraries, including Multi-GPU and TPU training, Mixed-precision and hierarchical configuration management.</p>
<p>We appreciate any kind of feedback or contribution. Feel free to proceed with small issues like bug fixes, documentation improvement. For major contributions and new features, please discuss with the collaborators in corresponding issues.</p>
<div class="section" id="why-should-i-use-openspeech">
<h1>Why should I use openspeech?<a class="headerlink" href="#why-should-i-use-openspeech" title="Permalink to this headline">¶</a></h1>
<ol class="simple">
<li><p>Easy-to-experiment with the famous ASR models.</p>
<ul class="simple">
<li><p>Supports 10+ models and is continuously updated.</p></li>
<li><p>Low barrier to entry for educators and practitioners.</p></li>
<li><p>Save time for researchers who want to conduct various experiments.</p></li>
</ul>
</li>
<li><p>Provides recipes for the most widely used languages, English, Chinese, and + Korean.</p>
<ul class="simple">
<li><p>LibriSpeech - 1,000 hours of English dataset most widely used in ASR tasks.</p></li>
<li><p>AISHELL-1 - 170 hours of Chinese Mandarin speech corpus.</p></li>
<li><p>KsponSpeech - 1,000 hours of Korean open-domain dialogue speech.</p></li>
</ul>
</li>
<li><p>Easily customize a model or a new dataset to your needs:</p>
<ul class="simple">
<li><p>The default hparams of the supported models are provided but can be easily adjusted.</p></li>
<li><p>Easily create a custom model by combining modules that are already provided.</p></li>
<li><p>If you want to use the new dataset, you only need to define a <code class="docutils literal notranslate"><span class="pre">pl.LightingDataModule</span></code> and <code class="docutils literal notranslate"><span class="pre">Vocabulary</span></code> classes.</p></li>
</ul>
</li>
<li><p>Audio processing</p>
<ul class="simple">
<li><p>Representative audio features such as Spectrogram, Mel-Spectrogram, Filter-Bank, and MFCC can be used easily.</p></li>
<li><p>Provides a variety of augmentation, including SpecAugment, Noise Injection, and Audio Joining.</p></li>
</ul>
</li>
</ol>
</div>
<div class="section" id="why-shouldn-t-i-use-openspeech">
<h1>Why shouldn’t I use openspeech?<a class="headerlink" href="#why-shouldn-t-i-use-openspeech" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p>This library provides code for learning ASR models, but does not provide APIs by pre-trained models.</p></li>
<li><p>We do not provide pre-training mechanisms such as Wav2vec 2.0. Because pre-training costs a lot of computation, computation optimization is very important, and this library does not provide that optimization.</p></li>
</ul>
</div>
<div class="section" id="model-architectures">
<h1>Model architectures<a class="headerlink" href="#model-architectures" title="Permalink to this headline">¶</a></h1>
<p>We support all the models below. Note that, the important concepts of the model have been implemented to match, but the details of the implementation may vary.</p>
<ol class="simple">
<li><p><a class="reference external" href="#"><strong>DeepSpeech2</strong></a> (from Baidu Research) released with paper <a class="reference external" href="https://arxiv.org/abs/1512.02595.pdf">Deep Speech 2: End-to-End Speech Recognition in
English and Mandarin</a>, by Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, Zhenyao Zhu.</p></li>
<li><p><a class="reference external" href="#"><strong>RNN-Transducer</strong></a> (from University of Toronto) released with paper <a class="reference external" href="https://arxiv.org/abs/1211.3711.pdf">Sequence Transduction with Recurrent Neural Networks</a>, by Alex Graves.</p></li>
<li><p><a class="reference external" href="https://sooftware.github.io/openspeech/Listen%20Attend%20Spell.html"><strong>Listen Attend Spell</strong></a> (from Carnegie Mellon University and Google Brain) released with paper <a class="reference external" href="https://arxiv.org/abs/1508.01211">Listen, Attend and Spell</a>, by William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals.</p></li>
<li><p><a class="reference external" href="#"><strong>Location-aware attention based Listen Attend Spell</strong></a> (from University of Wrocław and Jacobs University and Universite de Montreal ) released with paper <a class="reference external" href="https://arxiv.org/abs/1506.07503">Attention-Based Models for Speech Recognition</a>, by Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, Yoshua Bengio.</p></li>
<li><p><a class="reference external" href="#"><strong>Joint CTC-Attention based Listen Attend Spell</strong></a> (from Mitsubishi Electric Research Laboratories and Carnegie Mellon University) released with paper <a class="reference external" href="https://arxiv.org/abs/1609.06773">Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning</a>, by Suyoun Kim, Takaaki Hori, Shinji Watanabe.</p></li>
<li><p><a class="reference external" href="#"><strong>Deep CNN Encoder with Joint CTC-Attention Listen Attend Spell</strong></a> (from Mitsubishi Electric Research Laboratories and Massachusetts Institute of Technology and Carnegie Mellon University) released with paper <a class="reference external" href="https://arxiv.org/abs/1706.02737">Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM</a>, by Takaaki Hori, Shinji Watanabe, Yu Zhang, William Chan.</p></li>
<li><p><a class="reference external" href="#"><strong>Multi-head attention based Listen Attend Spell</strong></a> (from Google) released with paper <a class="reference external" href="https://arxiv.org/abs/1712.01769">State-of-the-art Speech Recognition With Sequence-to-Sequence Models</a>, by Chung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Rohit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, Anjuli Kannan, Ron J. Weiss, Kanishka Rao, Ekaterina Gonina, Navdeep Jaitly, Bo Li, Jan Chorowski, Michiel Bacchiani.</p></li>
<li><p><a class="reference external" href="#"><strong>Speech-Transformer</strong></a> (from University of Chinese Academy of Sciences and Institute of Automation and Chinese Academy of Sciences) released with paper <a class="reference external" href="https://ieeexplore.ieee.org/document/8462506">Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition</a>, by Linhao Dong; Shuang Xu; Bo Xu.</p></li>
<li><p><a class="reference external" href="#"><strong>VGG-Transformer</strong></a> (from Facebook AI Research) released with paper <a class="reference external" href="https://arxiv.org/abs/1904.11660">Transformers with convolutional context for ASR</a>, by Abdelrahman Mohamed, Dmytro Okhonko, Luke Zettlemoyer.</p></li>
<li><p><a class="reference external" href="https://github.com/sooftware/openspeech-dev/issues/151"><strong>Transformer with CTC</strong></a> (from NTT Communication Science Laboratories, Waseda University, Center for Language and Speech Processing, Johns Hopkins University) released with paper <a class="reference external" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1938.pdf">Improving Transformer-based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration</a>, by Shigeki Karita, Nelson Enrique Yalta Soplin, Shinji Watanabe, Marc Delcroix, Atsunori Ogawa, Tomohiro Nakatani.</p></li>
<li><p><a class="reference external" href="https://github.com/sooftware/openspeech-dev/issues/151"><strong>Joint CTC-Attention based Transformer</strong></a>(from NTT Corporation) released with paper <a class="reference external" href="https://www.isca-speech.org/archive/Interspeech_2020/pdfs/1223.pdf">Self-Distillation for Improving CTC-Transformer-based ASR Systems</a>, by Takafumi Moriya, Tsubasa Ochiai, Shigeki Karita, Hiroshi Sato, Tomohiro Tanaka, Takanori Ashihara, Ryo Masumura, Yusuke Shinohara, Marc Delcroix.</p></li>
<li><p><a class="reference external" href="#"><strong>Jasper</strong></a> (from NVIDIA and New York University) released with paper <a class="reference external" href="https://arxiv.org/pdf/1904.03288.pdf">Jasper: An End-to-End Convolutional Neural Acoustic Model</a>, by Jason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan Leary, Oleksii Kuchaiev, Jonathan M. Cohen, Huyen Nguyen, Ravi Teja Gadde.</p></li>
<li><p><a class="reference external" href="#"><strong>QuartzNet</strong></a> (from NVIDIA and Univ. of Illinois and Univ. of Saint Petersburg) released with paper <a class="reference external" href="https://arxiv.org/abs/1910.10261.pdf">QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions</a>, by Samuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, Yang Zhang.</p></li>
<li><p><a class="reference external" href="#"><strong>Transformer Transducer</strong></a> (from Facebook AI) released with paper <a class="reference external" href="https://arxiv.org/abs/1910.12977.pdf">Transformer-Transducer:
End-to-End Speech Recognition with Self-Attention</a>, by Ching-Feng Yeh, Jay Mahadeokar, Kaustubh Kalgaonkar, Yongqiang Wang, Duc Le, Mahaveer Jain, Kjell Schubert, Christian Fuegen, Michael L. Seltzer.</p></li>
<li><p><a class="reference external" href="#"><strong>Conformer</strong></a> (from Google) released with paper <a class="reference external" href="https://arxiv.org/abs/2005.08100">Conformer: Convolution-augmented Transformer for Speech Recognition</a>, by Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, Ruoming Pang.</p></li>
<li><p><a class="reference external" href="#"><strong>Conformer with CTC</strong></a> (from Northwestern Polytechnical University and University of Bordeaux and Johns Hopkins University and Human Dataware Lab and Kyoto University and NTT Corporation and Shanghai Jiao Tong University and  Chinese Academy of Sciences) released with paper <a class="reference external" href="https://arxiv.org/abs/2010.13956.pdf">RECENT DEVELOPMENTS ON ESPNET TOOLKIT BOOSTED BY CONFORMER</a>, by Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, Hirofumi Inaguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-Romero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun Wei, Wangyou Zhang, Yuekai Zhang.</p></li>
<li><p><a class="reference external" href="#"><strong>Conformer with LSTM Decoder</strong></a> (from IBM Research AI) released with paper <a class="reference external" href="https://arxiv.org/abs/2105.00982.pdf">On the limit of English conversational speech recognition</a>, by Zoltán Tüske, George Saon, Brian Kingsbury.</p></li>
</ol>
</div>
<div class="section" id="get-started">
<h1>Get Started<a class="headerlink" href="#get-started" title="Permalink to this headline">¶</a></h1>
<p>We use <a class="reference external" href="https://github.com/facebookresearch/hydra">Hydra</a> to control all the training configurations.
If you are not familiar with Hydra we recommend visiting the <a class="reference external" href="https://hydra.cc/">Hydra website</a>.
Generally, Hydra is an open-source framework that simplifies the development of research applications by providing the ability to create a hierarchical configuration dynamically.
If you want to know how we used Hydra, it’ll help if you read <a class="reference external" href="https://sooftware.github.io/openspeech/notes/hydra_configs.html">here</a>.</p>
<div class="section" id="supported-datasets">
<h2>Supported Datasets<a class="headerlink" href="#supported-datasets" title="Permalink to this headline">¶</a></h2>
<p>We supports <a class="reference external" href="https://www.openslr.org/12">LibriSpeech</a>, <a class="reference external" href="https://aihub.or.kr/aidata/105">KsponSpeech</a>, and <a class="reference external" href="https://www.openslr.org/33/">AISHELL-1</a>.</p>
<p>LibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech, prepared by Vassil Panayotov with the assistance of Daniel Povey. The data is derived from read audiobooks from the LibriVox project, and has been carefully segmented and aligned.</p>
<p>Aishell is an open-source Chinese Mandarin speech corpus published by Beijing Shell Shell Technology Co.,Ltd. 400 people from different accent areas in China are invited to participate in the recording, which is conducted in a quiet indoor environment using high fidelity microphone and downsampled to 16kHz.</p>
<p>KsponSpeech is a large-scale spontaneous speech corpus of Korean. This corpus contains 969 h of general open-domain dialog utterances, spoken by about 2000 native Korean speakers in a clean environment. All data were constructed by recording the dialogue of two people freely conversing on a variety of topics and manually transcribing the utterances. To start training, the KsponSpeech dataset must be prepared in advance. To download KsponSpeech, you needs permission from <a class="reference external" href="https://aihub.or.kr/">AI Hub</a>.</p>
</div>
<div class="section" id="manifest-file">
<h2>Manifest File<a class="headerlink" href="#manifest-file" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Manifest file format:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>LibriSpeech/test-other/8188/269288/8188-269288-0052.flac        ▁ANNIE &#39; S ▁MANNER ▁WAS ▁VERY ▁MYSTERIOUS       4039 20 5 531 17 84 2352
LibriSpeech/test-other/8188/269288/8188-269288-0053.flac        ▁ANNIE ▁DID ▁NOT ▁MEAN ▁TO ▁CONFIDE ▁IN ▁ANYONE ▁THAT ▁NIGHT ▁AND ▁THE ▁KIND EST ▁THING ▁WAS ▁TO ▁LEAVE ▁HER ▁A LONE    4039 99 35 251 9 4758 11 2454 16 199 6 4 323 200 255 17 9 370 30 10 492
LibriSpeech/test-other/8188/269288/8188-269288-0054.flac        ▁TIRED ▁OUT ▁LESLIE ▁HER SELF ▁DROPP ED ▁A SLEEP        1493 70 4708 30 115 1231 7 10 1706
LibriSpeech/test-other/8188/269288/8188-269288-0055.flac        ▁ANNIE ▁IS ▁THAT ▁YOU ▁SHE ▁CALL ED ▁OUT        4039 34 16 25 37 208 7 70
LibriSpeech/test-other/8188/269288/8188-269288-0056.flac        ▁THERE ▁WAS ▁NO ▁REPLY ▁BUT ▁THE ▁SOUND ▁OF ▁HURRY ING ▁STEPS ▁CAME ▁QUICK ER ▁AND ▁QUICK ER ▁NOW ▁AND ▁THEN ▁THEY ▁WERE ▁INTERRUPTED ▁BY ▁A ▁GROAN     57 17 56 1368 33 4 489 8 1783 14 1381 133 571 49 6 571 49 82 6 76 45 54 2351 44 10 3154
LibriSpeech/test-other/8188/269288/8188-269288-0057.flac        ▁OH ▁THIS ▁WILL ▁KILL ▁ME ▁MY ▁HEART ▁WILL ▁BREAK ▁THIS ▁WILL ▁KILL ▁ME 299 46 71 669 50 41 235 71 977 46 71 669 50
...
...
</pre></div>
</div>
</div>
<div class="section" id="training-examples">
<h2>Training examples<a class="headerlink" href="#training-examples" title="Permalink to this headline">¶</a></h2>
<p>You can simply train with LibriSpeech dataset like below:</p>
<ul class="simple">
<li><p>Example1: Train the <code class="docutils literal notranslate"><span class="pre">conformer-lstm</span></code> model with <code class="docutils literal notranslate"><span class="pre">filter-bank</span></code> features on GPU.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ python ./openspeech_cli/hydra_train.py \
    dataset=librispeech \
    dataset.dataset_download=True \
    dataset.dataset_path=$DATASET_PATH \
    dataset.manifest_file_path=$MANIFEST_FILE_PATH \  
    vocab=libri_subword \
    model=conformer_lstm \
    audio=fbank \
    lr_scheduler=warmup_reduce_lr_on_plateau \
    trainer=gpu \
    criterion=joint_ctc_cross_entropy
</pre></div>
</div>
<p>You can simply train with KsponSpeech dataset like below:</p>
<ul class="simple">
<li><p>Example2: Train the <code class="docutils literal notranslate"><span class="pre">listen-attend-spell</span></code> model with <code class="docutils literal notranslate"><span class="pre">mel-spectrogram</span></code> features On TPU:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ python ./openspeech_cli/hydra_train.py \
    dataset=ksponspeech \
    dataset.dataset_path=$DATASET_PATH \
    dataset.manifest_file_path=$MANIFEST_FILE_PATH \  
    dataset.test_dataset_path=$TEST_DATASET_PATH \
    dataset.test_manifest_dir=$TEST_MANIFEST_DIR \
    vocab=kspon_character \
    model=listen_attend_spell \
    audio=melspectrogram \
    lr_scheduler=warmup_reduce_lr_on_plateau \
    trainer=tpu \
    criterion=joint_ctc_cross_entropy
</pre></div>
</div>
<p>You can simply train with AISHELL-1 dataset like below:</p>
<ul class="simple">
<li><p>Example2: Train the <code class="docutils literal notranslate"><span class="pre">quartznet</span></code> model with <code class="docutils literal notranslate"><span class="pre">mfcc</span></code> features On GPU with FP16:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ python ./openspeech_cli/hydra_train.py \
    dataset=aishell \
    dataset.dataset_path=$DATASET_PATH \
    dataset.dataset_download=True \
    dataset.manifest_file_path=$MANIFEST_FILE_PATH \  
    vocab=aishell_character \
    model=quartznet15x5 \
    audio=mfcc \
    lr_scheduler=warmup_reduce_lr_on_plateau \
    trainer=gpu-fp16 \
    criterion=ctc
</pre></div>
</div>
</div>
</div>
<div class="section" id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h1>
<p>This project recommends Python 3.7 or higher.<br />I recommend creating a new virtual environment for this project (using virtual env or conda).</p>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>numpy: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">numpy</span></code> (Refer <a class="reference external" href="https://github.com/numpy/numpy">here</a> for problem installing Numpy).</p></li>
<li><p>pytorch: Refer to <a class="reference external" href="http://pytorch.org/">PyTorch website</a> to install the version w.r.t. your environment.</p></li>
<li><p>librosa: <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">-c</span> <span class="pre">conda-forge</span> <span class="pre">librosa</span></code> (Refer <a class="reference external" href="https://github.com/librosa/librosa">here</a> for problem installing librosa)</p></li>
<li><p>torchaudio: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torchaudio==0.6.0</span></code> (Refer <a class="reference external" href="https://github.com/pytorch/pytorch">here</a> for problem installing torchaudio)</p></li>
<li><p>sentencepiece: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">sentencepiece</span></code> (Refer <a class="reference external" href="https://github.com/google/sentencepiece">here</a> for problem installing sentencepiece)</p></li>
<li><p>pytorch-lightning: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pytorch-lightning</span></code> (Refer <a class="reference external" href="https://github.com/PyTorchLightning/pytorch-lightning">here</a> for problem installing pytorch-lightning)</p></li>
<li><p>hydra: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">hydra-core</span> <span class="pre">--upgrade</span></code> (Refer <a class="reference external" href="https://github.com/facebookresearch/hydra">here</a> for problem installing hydra)</p></li>
<li><p>warp-rnnt: Refer to <a class="reference external" href="https://github.com/1ytic/warp-rnnt">warp-rnnt page</a> to install the library.</p></li>
<li><p>ctcdecode: Refer to <a class="reference external" href="https://github.com/parlance/ctcdecode">ctcdecode page</a> to install the library.</p></li>
</ul>
</div>
<div class="section" id="install-from-source">
<h2>Install from source<a class="headerlink" href="#install-from-source" title="Permalink to this headline">¶</a></h2>
<p>Currently I only support installation from source code using setuptools. Checkout the source code and run the<br />following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ./install.sh
</pre></div>
</div>
</div>
<div class="section" id="install-apex-for-16-bit-training">
<h2>Install Apex (for 16-bit training)<a class="headerlink" href="#install-apex-for-16-bit-training" title="Permalink to this headline">¶</a></h2>
<p>For faster training install NVIDIA’s apex library:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ git clone https://github.com/NVIDIA/apex
$ cd apex

# ------------------------
# OPTIONAL: on your cluster you might need to load CUDA 10 or 9
# depending on how you installed PyTorch

# see available modules
module avail

# load correct CUDA before install
module load cuda-10.0
# ------------------------

# make sure you&#39;ve loaded a cuda version &gt; 4.0 and &lt; 7.0
module load gcc-6.1.0

$ pip install -v --no-cache-dir --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext&quot; ./
</pre></div>
</div>
</div>
</div>
<div class="section" id="troubleshoots-and-contributing">
<h1>Troubleshoots and Contributing<a class="headerlink" href="#troubleshoots-and-contributing" title="Permalink to this headline">¶</a></h1>
<p>If you have any questions, bug reports, and feature requests, please <a class="reference external" href="https://github.com/sooftware/openspeech/issues">open an issue</a> on Github.</p>
<p>We appreciate any kind of feedback or contribution.  Feel free to proceed with small issues like bug fixes, documentation improvement.  For major contributions and new features, please discuss with the collaborators in corresponding issues.</p>
<div class="section" id="code-style">
<h2>Code Style<a class="headerlink" href="#code-style" title="Permalink to this headline">¶</a></h2>
<p>We follow <a class="reference external" href="https://www.python.org/dev/peps/pep-0008/">PEP-8</a> for code style. Especially the style of docstrings is important to generate documentation.</p>
</div>
<div class="section" id="license">
<h2>License<a class="headerlink" href="#license" title="Permalink to this headline">¶</a></h2>
<p>This project is licensed under the MIT LICENSE - see the <a class="reference external" href="https://github.com/sooftware/openspeech/blob/master/LICENSE">LICENSE.md</a> file for details</p>
</div>
</div>
<div class="section" id="citation">
<h1>Citation<a class="headerlink" href="#citation" title="Permalink to this headline">¶</a></h1>
<p>If you use the system for academic work, please cite:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@GITHUB</span><span class="p">{</span><span class="mi">2021</span><span class="o">-</span><span class="n">openspeech</span><span class="p">,</span>
  <span class="n">author</span>       <span class="o">=</span> <span class="p">{</span><span class="n">Kim</span><span class="p">,</span> <span class="n">Soohwan</span> <span class="ow">and</span> <span class="n">Ha</span><span class="p">,</span> <span class="n">Sangchun</span> <span class="ow">and</span> <span class="n">Cho</span><span class="p">,</span> <span class="n">Soyoung</span><span class="p">},</span>
  <span class="n">author</span> <span class="n">email</span> <span class="o">=</span> <span class="p">{</span><span class="n">sh951011</span><span class="nd">@gmail</span><span class="o">.</span><span class="n">com</span><span class="p">,</span> <span class="n">seomk9896</span><span class="nd">@naver</span><span class="o">.</span><span class="n">com</span><span class="p">,</span> <span class="n">soyoung</span><span class="o">.</span><span class="n">cho</span><span class="nd">@kaist</span><span class="o">.</span><span class="n">ac</span><span class="o">.</span><span class="n">kr</span><span class="p">}</span>
  <span class="n">title</span>        <span class="o">=</span> <span class="p">{</span><span class="n">Openspeech</span><span class="p">:</span> <span class="n">Open</span><span class="o">-</span><span class="n">Source</span> <span class="n">Toolkit</span> <span class="k">for</span> <span class="n">End</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">End</span> <span class="n">Speech</span> <span class="n">Recognition</span><span class="p">},</span>
  <span class="n">howpublished</span> <span class="o">=</span> <span class="p">{</span>\<span class="n">url</span><span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">sooftware</span><span class="o">/</span><span class="n">openspeech</span><span class="p">}},</span>
  <span class="n">docs</span>         <span class="o">=</span> <span class="p">{</span>\<span class="n">url</span><span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">sooftware</span><span class="o">.</span><span class="n">github</span><span class="o">.</span><span class="n">io</span><span class="o">/</span><span class="n">openspeech</span><span class="p">}},</span>
  <span class="n">year</span>         <span class="o">=</span> <span class="p">{</span><span class="mi">2021</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="hydra_configs.html" class="btn btn-neutral float-right" title="Openspeech’s Hydra configuration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../index.html" class="btn btn-neutral float-left" title="Welcome to Openspeech’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Kim, Soohwan and Ha, Sangchun and Cho, Soyoung.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>